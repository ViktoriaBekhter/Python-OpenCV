{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ViktoriaBekhter/Python-OpenCV-/blob/main/OpenCV_%D0%9A%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2_%D1%81_%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5%D0%BC_NLTK_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhQ_OSI9qCk-"
      },
      "source": [
        "Инструментарий:\n",
        "\n",
        "\n",
        "*   Python\n",
        "*   NLTK\n",
        "*   SKLearn\n",
        "*   Pandas\n",
        "*   NumPy\n",
        "*   PyTorch\n",
        "*   TensorFlow\n",
        "*   Keras\n",
        "*   natasha\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V6gpOc8AKrw"
      },
      "source": [
        "# Распознавание именованных сущностей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7_Cq-liKjyi"
      },
      "source": [
        "Named Entity Recognition (NER) — это задача в области обработки естественного языка (NLP), направленная на выделение и классификацию именованных сущностей в тексте, таких как имена людей, названия организаций, даты, местоположения, суммы денег и другие типы специфических объектов. NER является важным компонентом многих NLP-приложений, таких как извлечение информации, анализ тональности, вопросно-ответные системы и многие другие.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb_tJh5AKlKq"
      },
      "source": [
        "Проведем выделение именованных сущностей с помощью библиотеки NLTK.\n",
        "Используем для этого утилиту ne_chunk распознавания именованных\n",
        "сущностей из NLTK, которая создает вложенную древовидную структуру\n",
        "с синтаксическими категориями и тегами частей речи, содержащимися в каждом предложении."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGlEjxVWISYB"
      },
      "outputs": [],
      "source": [
        "# импорт библиотеки\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K6Wf-qDION7",
        "outputId": "05e11652-b18f-4780-8f14-e5f009c9a046"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# загрузка массивов\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jte6-2HsGpi4",
        "outputId": "f02981c8-72be-4629-db66-e901f04f34bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PERSON Elon/NNP)\n",
            "  (PERSON Musk/NNP)\n",
            "  ’/NNP\n",
            "  s/NN\n",
            "  (ORGANIZATION SpaceX/NNP)\n",
            "  has/VBZ\n",
            "  successfully/RB\n",
            "  carried/VBN\n",
            "  out/RP\n",
            "  the/DT\n",
            "  longest/JJS\n",
            "  test/NN\n",
            "  flight/NN\n",
            "  of/IN\n",
            "  its/PRP$\n",
            "  massive/JJ\n",
            "  Starship/NN\n",
            "  rocket/NN\n",
            "  ,/,\n",
            "  but/CC\n",
            "  it/PRP\n",
            "  disintegrated/VBD\n",
            "  on/IN\n",
            "  its/PRP$\n",
            "  return/NN\n",
            "  to/TO\n",
            "  (GPE Earth/NNP)\n",
            "  ./.\n",
            "  The/DT\n",
            "  module/NN\n",
            "  was/VBD\n",
            "  destroyed/VBN\n",
            "  while/IN\n",
            "  approaching/VBG\n",
            "  its/PRP$\n",
            "  landing/NN\n",
            "  point/NN\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (GPE Indian/JJ)\n",
            "  (LOCATION Ocean/NNP))\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Elon Musk’s SpaceX has successfully carried out the longest test flight of its massive\n",
        "  Starship rocket, but it disintegrated on its return to Earth.\n",
        "  The module was destroyed while approaching its landing point in the Indian Ocean\"\"\"\n",
        "\n",
        "# токенизация\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Разметка частей речи\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "# Выделение именованных сущностей на основе дерева разметки\n",
        "entities = nltk.chunk.ne_chunk(tagged)\n",
        "print(entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uSmP9jzNovL",
        "outputId": "b992c85a-d656-4a74-ad38-9583f8117a10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORGANIZATION --> SpaceX\n",
            "GPE --> Earth\n",
            "GPE --> Indian\n"
          ]
        }
      ],
      "source": [
        "# Вывод найденных сущностей\n",
        "for entity in entities:\n",
        "    if hasattr(entity, 'label') and entity.label() == 'ORGANIZATION':\n",
        "        print(entity.label(),'-->', ''.join(c[0] for c in entity))\n",
        "    elif hasattr(entity, 'label') and entity.label() == 'GPE':\n",
        "        print(entity.label(), '-->',''.join(c[0] for c in entity))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgDNt4o7O_CB"
      },
      "source": [
        " Расширить вывод для отображения людей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO5noFX4fMDG",
        "outputId": "1ee97738-cfe5-4816-b7f4-364b2f194b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PERSON --> Elon\n",
            "PERSON --> Musk\n",
            "ORGANIZATION --> SpaceX\n",
            "GPE --> Earth\n",
            "GPE --> Indian\n"
          ]
        }
      ],
      "source": [
        "for entity in entities:\n",
        "    if hasattr(entity, 'label'):\n",
        "        if entity.label() == 'ORGANIZATION':\n",
        "            print(entity.label(), '-->', ''.join(c[0] for c in entity))\n",
        "        elif entity.label() == 'GPE':\n",
        "            print(entity.label(), '-->', ''.join(c[0] for c in entity))\n",
        "        elif entity.label() == 'PERSON':\n",
        "            print(entity.label(), '-->', ''.join(c[0] for c in entity))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGLT3gDZPKhR"
      },
      "source": [
        " Найти именованные сущности в произвольном английском тексте"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_7APbBjf_WT",
        "outputId": "8dfefc77-d293-4bf9-e78d-127dded35b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PERSON Elon/NNP)\n",
            "  (PERSON Musk/NNP)\n",
            "  ’/NNP\n",
            "  s/NN\n",
            "  (ORGANIZATION SpaceX/NNP)\n",
            "  has/VBZ\n",
            "  successfully/RB\n",
            "  carried/VBN\n",
            "  out/RP\n",
            "  the/DT\n",
            "  longest/JJS\n",
            "  test/NN\n",
            "  flight/NN\n",
            "  of/IN\n",
            "  its/PRP$\n",
            "  massive/JJ\n",
            "  Starship/NN\n",
            "  rocket/NN\n",
            "  ,/,\n",
            "  but/CC\n",
            "  it/PRP\n",
            "  disintegrated/VBD\n",
            "  on/IN\n",
            "  its/PRP$\n",
            "  return/NN\n",
            "  to/TO\n",
            "  (GPE Earth/NNP)\n",
            "  ./.\n",
            "  The/DT\n",
            "  module/NN\n",
            "  was/VBD\n",
            "  destroyed/VBN\n",
            "  while/IN\n",
            "  approaching/VBG\n",
            "  its/PRP$\n",
            "  landing/NN\n",
            "  point/NN\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (GPE Indian/JJ)\n",
            "  (LOCATION Ocean/NNP))\n",
            "PERSON --> Elon\n",
            "PERSON --> Musk\n",
            "ORGANIZATION --> SpaceX\n",
            "GPE --> Earth\n",
            "GPE --> Indian\n",
            "(S\n",
            "  I/PRP\n",
            "  like/IN\n",
            "  spending/NN\n",
            "  time/NN\n",
            "  with/IN\n",
            "  my/PRP$\n",
            "  friends/NNS\n",
            "  ./.\n",
            "  We/PRP\n",
            "  often/RB\n",
            "  visit/VBP\n",
            "  each/DT\n",
            "  other/JJ\n",
            "  ./.\n",
            "  I/PRP\n",
            "  can/MD\n",
            "  talk/VB\n",
            "  with/IN\n",
            "  them/PRP\n",
            "  for/IN\n",
            "  hours/NNS\n",
            "  ./.\n",
            "  They/PRP\n",
            "  can/MD\n",
            "  help/VB\n",
            "  me/PRP\n",
            "  and/CC\n",
            "  support/VB\n",
            "  me/PRP\n",
            "  in/IN\n",
            "  any/DT\n",
            "  situation/NN\n",
            "  ./.\n",
            "  I/PRP\n",
            "  can/MD\n",
            "  say/VB\n",
            "  the/DT\n",
            "  same/JJ\n",
            "  about/IN\n",
            "  my/PRP$\n",
            "  parents/NNS\n",
            "  ,/,\n",
            "  with/IN\n",
            "  whom/WP\n",
            "  I/PRP\n",
            "  live/VBP\n",
            "  ./.\n",
            "  My/PRP$\n",
            "  mother/NN\n",
            "  (PERSON Ann/NNP)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  very/RB\n",
            "  wise/JJ\n",
            "  woman/NN\n",
            "  ./.)\n",
            "PERSON --> Ann\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Токенизация\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Разметка частей речи\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "# Выделение именованных сущностей на основе дерева разметки\n",
        "entities = nltk.chunk.ne_chunk(tagged)\n",
        "print(entities)\n",
        "\n",
        "# Вывод найденных сущностей\n",
        "for entity in entities:\n",
        "    if hasattr(entity, 'label'):\n",
        "        if entity.label() == 'ORGANIZATION':\n",
        "            print(entity.label(), '-->', ''.join(c[0] for c in entity))\n",
        "        elif entity.label() == 'GPE':\n",
        "            print(entity.label(), '-->', ''.join(c[0] for c in entity))\n",
        "        elif entity.label() == 'PERSON':\n",
        "            print(entity.label(), '-->', ''.join(c[0] for c in entity))\n",
        "\n",
        "text = \"\"\"I like spending time with my friends. We often visit each other. I can talk with them for hours.\n",
        "They can help me and support me in any situation. I can say the same about my parents, with whom I live.\n",
        "My mother Ann is a very wise woman.\"\"\"\n",
        "\n",
        "# Токенизация\n",
        "tokens_new = nltk.word_tokenize(text)\n",
        "\n",
        "# Разметка частей речи\n",
        "tagged_new = nltk.pos_tag(tokens_new)\n",
        "\n",
        "# Выделение именованных сущностей на основе дерева разметки\n",
        "entities_new = nltk.chunk.ne_chunk(tagged_new)\n",
        "print(entities_new)\n",
        "\n",
        "# Вывод найденных сущностей\n",
        "for entity in entities_new:\n",
        "    if hasattr(entity, 'label'):\n",
        "        if entity.label() == 'ORGANIZATION':\n",
        "            print(entity.label(), '-->', ''.join(c[0] for c in entity))\n",
        "        elif entity.label() == 'GPE':\n",
        "            print(entity.label(), '-->', ''.join(c[0] for c in entity))\n",
        "        elif entity.label() == 'PERSON':\n",
        "            print(entity.label(), '-->', ''.join(c[0] for c in entity))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhka0YWQJcTm"
      },
      "source": [
        "Для выделения именованных сущностей на русском языке применим библиотеку Natasha.\n",
        "https://vc.ru/newtechaudit/109109-natasha-instrument-dlya-izvlecheniya-imenovannyh-sushchnostey-iz-russkih-tekstov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tMRFUAPSxgk"
      },
      "source": [
        "Установить библиотеку natasha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBdGHxEBW-HB",
        "outputId": "6462dea3-27a0-4646-f453-e89c183ac178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy2 (from natasha)\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from navec>=0.9.0->natasha) (1.25.2)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2->natasha)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2->natasha)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2->natasha)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Building wheels for collected packages: docopt, intervaltree\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=21128a915b40461241ecdc74fca6a7c0af45dcee2611ba1a6065eb86896f296f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26096 sha256=0b3eb1bc1c5a7b95fbe6e9543d78d6dbeaeedb8ee406a70274965ee0e5235a33\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built docopt intervaltree\n",
            "Installing collected packages: razdel, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.6.0 yargy-0.16.0\n"
          ]
        }
      ],
      "source": [
        "pip install natasha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5HAOJOjJam8"
      },
      "outputs": [],
      "source": [
        "# импорт необходимых модулей библиотеки\n",
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "\n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "    DatesExtractor,\n",
        "    MoneyExtractor,\n",
        "    AddrExtractor,\n",
        "\n",
        "    Doc\n",
        ")\n",
        "# создание объектов для этапов проведения анализа текстов\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "# выделение имен\n",
        "names_extractor = NamesExtractor(morph_vocab)\n",
        "\n",
        "\n",
        "text = \"\"\"5 апреля в МБОУ СОШ № 15 г. Батайска были организованы тематические мероприятия в университетских профильных классах начальной школы.\n",
        " Для обучающихся IT класса ассистент кафедры Информационных систем и прикладной информатики Алексей Серов\n",
        " совместно с заместителем декана по воспитательной работе факультета Компьютерных технологий и информационной безопасности\n",
        " Екатериной Лозиной провели урок «Я в виртуальном мире».\n",
        " Школьники познакомились с профессиями IT сферы, спецификой и особенностями работы специалистов этой сферы.\"\"\"\n",
        "\n",
        "# создание объекта-документа\n",
        "doc=Doc(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW0yam8cYmXP",
        "outputId": "72c904c8-ebb6-4260-a6c1-2879a4292e07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DocToken(stop=1, text='5')\n",
            "DocToken(start=2, stop=8, text='апреля')\n",
            "DocToken(start=9, stop=10, text='в')\n",
            "DocToken(start=11, stop=15, text='МБОУ')\n",
            "DocToken(start=16, stop=19, text='СОШ')\n",
            "DocToken(start=20, stop=21, text='№')\n",
            "DocToken(start=22, stop=24, text='15')\n",
            "DocToken(start=25, stop=26, text='г')\n",
            "DocToken(start=26, stop=27, text='.')\n",
            "DocToken(start=28, stop=36, text='Батайска')\n",
            "DocToken(start=37, stop=41, text='были')\n",
            "DocToken(start=42, stop=54, text='организованы')\n",
            "DocToken(start=55, stop=67, text='тематические')\n",
            "DocToken(start=68, stop=79, text='мероприятия')\n",
            "DocToken(start=80, stop=81, text='в')\n"
          ]
        }
      ],
      "source": [
        "# Токенизация\n",
        "doc.segment(segmenter)\n",
        "print(*doc.tokens[:15], sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o04YT3uZLMg",
        "outputId": "44331518-a8af-4f33-fa12-6d0cb23f46d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   5 ADJ\n",
            "              апреля NOUN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
            "                   в ADP\n",
            "                МБОУ PROPN|Animacy=Inan|Case=Loc|Gender=Masc|Number=Sing\n",
            "                 СОШ PROPN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing\n",
            "                   № SYM\n",
            "                  15 NUM\n",
            "                   г NOUN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
            "                   . PUNCT\n",
            "            Батайска PROPN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
            "                были AUX|Aspect=Imp|Mood=Ind|Number=Plur|Tense=Past|VerbForm=Fin|Voice=Act\n",
            "        организованы VERB|Aspect=Perf|Number=Plur|Tense=Past|Variant=Short|VerbForm=Part|Voice=Pass\n",
            "        тематические ADJ|Case=Nom|Degree=Pos|Number=Plur\n",
            "         мероприятия NOUN|Animacy=Inan|Case=Nom|Gender=Neut|Number=Plur\n",
            "                   в ADP\n",
            "     университетских ADJ|Case=Loc|Degree=Pos|Number=Plur\n",
            "          профильных ADJ|Case=Loc|Degree=Pos|Number=Plur\n",
            "             классах NOUN|Animacy=Inan|Case=Loc|Gender=Masc|Number=Plur\n",
            "           начальной ADJ|Case=Gen|Degree=Pos|Gender=Fem|Number=Sing\n",
            "               школы NOUN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n",
            "                   . PUNCT\n"
          ]
        }
      ],
      "source": [
        "# морфологический анализ и разметка частей речи\n",
        "doc.tag_morph(morph_tagger)\n",
        "doc.sents[0].morph.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjEvfvKfZXXd",
        "outputId": "f02e14c4-f3d2-4838-b0b4-838a00d5a259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DocToken(stop=1, text='5', pos='ADJ', lemma='5')\n",
            "DocToken(start=2, stop=8, text='апреля', pos='NOUN', feats=<Inan,Gen,Masc,Sing>, lemma='апрель')\n",
            "DocToken(start=9, stop=10, text='в', pos='ADP', lemma='в')\n",
            "DocToken(start=11, stop=15, text='МБОУ', pos='PROPN', feats=<Inan,Loc,Masc,Sing>, lemma='мбоу')\n",
            "DocToken(start=16, stop=19, text='СОШ', pos='PROPN', feats=<Inan,Gen,Neut,Sing>, lemma='сош')\n"
          ]
        }
      ],
      "source": [
        "# Лемматизация\n",
        "# Основана на использовании библиотеки pymorphy\n",
        "for token in doc.tokens:\n",
        "  token.lemmatize(morph_vocab)\n",
        "print(*doc.tokens[:5],sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_ng7FdCadxN",
        "outputId": "aa21d0d6-896f-4707-9b9b-9add66fbfc1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "┌──────────► 5               obl\n",
            "│            апреля          \n",
            "│         ┌► в               case\n",
            "│ ┌────►┌─└─ МБОУ            obl\n",
            "│ │   ┌─└►┌─ СОШ             nmod\n",
            "│ │ ┌─│ ┌─└► №               nummod:entity\n",
            "│ │ │ │ └──► 15              appos\n",
            "│ │ │ └────► г               nmod\n",
            "│ │ │   ┌──► .               punct\n",
            "│ │ └──►│    Батайска        appos\n",
            "│ │     │ ┌► были            aux:pass\n",
            "└─└─────└─└─ организованы    \n",
            "│       │ ┌► тематические    amod\n",
            "│   ┌───└►└─ мероприятия     nsubj:pass\n",
            "│   │ ┌────► в               case\n",
            "│   │ │ ┌──► университетских amod\n",
            "│   │ │ │ ┌► профильных      amod\n",
            "│   └►└─└─└─ классах         nmod\n",
            "│     │   ┌► начальной       amod\n",
            "│     └──►└─ школы           nmod\n",
            "└──────────► .               punct\n",
            "                  ┌► Для            case\n",
            "┌──────────────►┌─└─ обучающихся    obl\n",
            "│               └──► IT             nmod\n",
            "│         ┌────────► класса         nmod\n",
            "│         │       ┌► ассистент      amod\n",
            "│ ┌►┌─────│   ┌─┌─└─ кафедры        nmod\n",
            "│ │ │     │   │ │ ┌► Информационных amod\n",
            "│ │ │   ┌─│   │ └►└─ систем         nmod\n",
            "│ │ │   │ │   │   ┌► и              cc\n",
            "│ │ │   │ │   │ ┌►└─ прикладной     amod\n",
            "│ │ │   │ │ ┌►│ └─── информатики    nmod\n",
            "│ │ │   │ │ │ └──►┌─ Алексей        appos\n",
            "│ │ │   │ │ │     └► Серов          flat:name\n",
            "│ │ │ ┌►│ │ │   ┌─── совместно      advmod\n",
            "│ │ │ │ │ │ │   │ ┌► с              case\n",
            "│ │ │ │ │ └─│   └►└─ заместителем   obl\n",
            "│ │ │ │ │   │ ┌─└──► декана         nmod\n",
            "│ │ │ │ │   │ │ ┌──► по             case\n",
            "│ │ │ │ │   │ │ │ ┌► воспитательной amod\n",
            "│ │ │ │ │   │ └►└─└─ работе         nmod\n",
            "│ │ │ │ │   └───└►┌─ факультета     nmod\n",
            "│ │ │ │ └────────►│  Компьютерных   amod\n",
            "│ │ │ │       ┌───└► технологий     nmod\n",
            "│ │ │ │       │ ┌──► и              cc\n",
            "│ │ │ │       │ │ ┌► информационной amod\n",
            "│ │ │ │       └►└─└─ безопасности   conj\n",
            "│ │ └►│           ┌─ Екатериной     appos\n",
            "│ │   │           └► Лозиной        flat:name\n",
            "└─│   └─┌─────────┌─ провели        \n",
            "  └─────│ ┌───────└► урок           obj\n",
            "        │ │ ┌──────► «              punct\n",
            "        │ │ │ ┌────► Я              nsubj\n",
            "        │ │ │ │ ┌──► в              case\n",
            "        │ │ │ │ │ ┌► виртуальном    amod\n",
            "        │ └►└─└─└─└─ мире           nmod\n",
            "        │       └──► »              punct\n",
            "        └──────────► .              punct\n"
          ]
        }
      ],
      "source": [
        "# Построение синтаксического дерева\n",
        "# Опирается на предыдущие шаги\n",
        "\n",
        "\n",
        "\n",
        "doc.parse_syntax(syntax_parser)\n",
        "doc.sents[0].syntax.print()\n",
        "doc.sents[1].syntax.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsB2Sh8abWyu",
        "outputId": "f62b0bc2-2413-4a87-dba9-98dd4adc2283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DocSpan(start=11, stop=19, type='ORG', text='МБОУ СОШ', tokens=[...])\n",
            "DocSpan(start=28, stop=36, type='LOC', text='Батайска', tokens=[...])\n",
            "DocSpan(start=179, stop=225, type='ORG', text='Информационных систем и прикладной информатики', tokens=[...])\n",
            "DocSpan(start=226, stop=239, type='PER', text='Алексей Серов', tokens=[...])\n",
            "DocSpan(start=364, stop=382, type='PER', text='Екатериной Лозиной', tokens=[...])\n",
            "5 апреля в МБОУ СОШ № 15 г. Батайска были организованы тематические \n",
            "           ORG─────         LOC─────                                \n",
            "мероприятия в университетских профильных классах начальной школы.\n",
            " Для обучающихся IT класса ассистент кафедры Информационных систем и \n",
            "                                             ORG─────────────────────\n",
            "прикладной информатики Алексей Серов\n",
            "────────────────────── PER──────────\n",
            " совместно с заместителем декана по воспитательной работе факультета \n",
            "Компьютерных технологий и информационной безопасности\n",
            " Екатериной Лозиной провели урок «Я в виртуальном мире».\n",
            " PER───────────────                                     \n",
            " Школьники познакомились с профессиями IT сферы, спецификой и \n",
            "особенностями работы специалистов этой сферы.\n"
          ]
        }
      ],
      "source": [
        "doc.tag_ner(ner_tagger)\n",
        "print(*doc.spans[:5],sep='\\n')\n",
        "doc.ner.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFSv9rpHcXsj",
        "outputId": "e1b822ec-54f4-4b35-e48a-bc53c93b30e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Алексей Серов\n",
            "[Slot(key='first', value='Алексей'), Slot(key='last', value='Серов')]\n",
            "Екатериной Лозиной\n",
            "[Slot(key='first', value='Екатерина'), Slot(key='last', value='Лозина')]\n"
          ]
        }
      ],
      "source": [
        "# Список персон\n",
        "\n",
        "# нормализация выделенных сущностей\n",
        "for span in doc.spans:\n",
        "  span.normalize(morph_vocab)\n",
        "\n",
        "# вывод\n",
        "for span in doc.spans:\n",
        "  if span.type == 'PER':\n",
        "    print(span.text)\n",
        "    span.extract_fact(names_extractor)\n",
        "    print(span.fact.slots)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnkw8MiZfFfi",
        "outputId": "13a91009-fd90-4677-e6b7-2303c2b8558d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Match(start=25, stop=36, fact=AddrPart(value='Батайска', type='город'))\n",
            "Match(start=234, stop=239, fact=AddrPart(value='Серов', type=None))\n"
          ]
        }
      ],
      "source": [
        "# помимо выделения имен имееется возможность определения дат, адресов, денежных сумм\n",
        "addr_extractor=AddrExtractor(morph_vocab)\n",
        "for x in addr_extractor(text):\n",
        "  print(x)\n",
        "# результат содержит погрешность"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hiML-_lf-3W"
      },
      "source": [
        " Выделить даты в приведенном тексте. Используем объект класса DateExtractor\n",
        "https://nbviewer.org/github/natasha/natasha/blob/master/docs.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TfGoLW3ohfiY"
      },
      "outputs": [],
      "source": [
        "from natasha import DatesExtractor, MorphVocab\n",
        "\n",
        "text = 'Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав о решении властей Львовской области объявить 2019 год годом лидера запрещенной в России Организации украинских националистов (ОУН) Степана Бандеры. Свое заявление он разместил в Twitter. «Я не могу понять, как прославление тех, кто непосредственно принимал участие в ужасных антисемитских преступлениях, помогает бороться с антисемитизмом и ксенофобией. Украина не должна забывать о преступлениях, совершенных против украинских евреев, и никоим образом не отмечать их через почитание их исполнителей», — написал дипломат. 11 декабря Львовский областной совет принял решение провозгласить 2019 год в регионе годом Степана Бандеры в связи с празднованием 110-летия со дня рождения лидера ОУН (Бандера родился 1 января 1909 года). В июле аналогичное решение принял Житомирский областной совет. В начале месяца с предложением к президенту страны Петру Порошенко вернуть Бандере звание Героя Украины обратились депутаты Верховной Рады. Парламентарии уверены, что признание Бандеры национальным героем поможет в борьбе с подрывной деятельностью против Украины в информационном поле, а также остановит «распространение мифов, созданных российской пропагандой». Степан Бандера (1909-1959) был одним из лидеров Организации украинских националистов, выступающей за создание независимого государства на территориях с украиноязычным населением. В 2010 году в период президентства Виктора Ющенко Бандера был посмертно признан Героем Украины, однако впоследствии это решение было отменено судом.'\n",
        "\n",
        "morph_vocab = MorphVocab()\n",
        "extractor = DatesExtractor(morph_vocab)\n",
        "matches = extractor(text)\n",
        "\n",
        "for match in matches:\n",
        "    start = match.span[0]\n",
        "    end = match.span[1]\n",
        "    print(text[start:end])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3V3_CoEeTBR"
      },
      "source": [
        " Провести с помощью Natasha анализ произвольного текста. Построить синтаксическое дерево. Выделить именованные сущности. Получить список персон, адресов, дат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeG9N_wEjk2g",
        "outputId": "e10be163-f93f-44e0-ef40-ae6fe6bbd6af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Doc(text='Посол Израиля на Украине Йоэль Лион признался, чт...)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав о решении властей Львовской области объявить 2019 год годом лидера запрещенной в России Организации украинских националистов (ОУН) Степана Бандеры. Свое заявление он разместил в Twitter. «Я не могу понять, как прославление тех, кто непосредственно принимал участие в ужасных антисемитских преступлениях, помогает бороться с антисемитизмом и ксенофобией. Украина не должна забывать о преступлениях, совершенных против украинских евреев, и никоим образом не отмечать их через почитание их исполнителей», — написал дипломат. 11 декабря Львовский областной совет принял решение провозгласить 2019 год в регионе годом Степана Бандеры в связи с празднованием 110-летия со дня рождения лидера ОУН (Бандера родился 1 января 1909 года). В июле аналогичное решение принял Житомирский областной совет. В начале месяца с предложением к президенту страны Петру Порошенко вернуть Бандере звание Героя Украины обратились депутаты Верховной Рады. Парламентарии уверены, что признание Бандеры национальным героем поможет в борьбе с подрывной деятельностью против Украины в информационном поле, а также остановит «распространение мифов, созданных российской пропагандой». Степан Бандера (1909-1959) был одним из лидеров Организации украинских националистов, выступающей за создание независимого государства на территориях с украиноязычным населением. В 2010 году в период президентства Виктора Ющенко Бандера был посмертно признан Героем Украины, однако впоследствии это решение было отменено судом. '\n",
        "doc = Doc(text)\n",
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "n380_wa6j7zc",
        "outputId": "a9d97487-6503-4931-d06b-acb3e2ef25e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Doc(text='Посол Израиля на Украине Йоэль Лион признался, чт..., tokens=[...], sents=[...])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[DocSent(stop=218, text='Посол Израиля на Украине Йоэль Лион признался, чт..., tokens=[...]),\n",
              " DocSent(start=219, stop=257, text='Свое заявление он разместил в Twitter.', tokens=[...])]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[DocToken(stop=5, text='Посол'),\n",
              " DocToken(start=6, stop=13, text='Израиля'),\n",
              " DocToken(start=14, stop=16, text='на'),\n",
              " DocToken(start=17, stop=24, text='Украине'),\n",
              " DocToken(start=25, stop=30, text='Йоэль')]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "doc.segment(segmenter)\n",
        "display(doc)\n",
        "display(doc.sents[:2])\n",
        "display(doc.tokens[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Gg03H8ypkfFU",
        "outputId": "362a992f-abc4-4a40-afbc-659cb690fb36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[DocToken(stop=5, text='Посол', id='1_1', head_id='1_7', rel='nsubj', pos='NOUN', feats=<Anim,Nom,Masc,Sing>),\n",
              " DocToken(start=6, stop=13, text='Израиля', id='1_2', head_id='1_1', rel='nmod', pos='PROPN', feats=<Inan,Gen,Masc,Sing>),\n",
              " DocToken(start=14, stop=16, text='на', id='1_3', head_id='1_4', rel='case', pos='ADP'),\n",
              " DocToken(start=17, stop=24, text='Украине', id='1_4', head_id='1_1', rel='nmod', pos='PROPN', feats=<Inan,Loc,Fem,Sing>),\n",
              " DocToken(start=25, stop=30, text='Йоэль', id='1_5', head_id='1_1', rel='appos', pos='PROPN', feats=<Anim,Nom,Masc,Sing>)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "doc.tag_morph(morph_tagger)\n",
        "doc.parse_syntax(syntax_parser)\n",
        "display(doc.tokens[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "2BoFAXKUkIjk",
        "outputId": "8dd8e7a7-1792-4ac0-e10b-37a7847f53de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[DocSpan(start=6, stop=13, type='LOC', text='Израиля', tokens=[...]),\n",
              " DocSpan(start=17, stop=24, type='LOC', text='Украине', tokens=[...]),\n",
              " DocSpan(start=25, stop=35, type='PER', text='Йоэль Лион', tokens=[...]),\n",
              " DocSpan(start=89, stop=106, type='LOC', text='Львовской области', tokens=[...]),\n",
              " DocSpan(start=152, stop=158, type='LOC', text='России', tokens=[...])]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "doc.tag_ner(ner_tagger)\n",
        "display(doc.spans[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weyEHI7bkJzn",
        "outputId": "ef618334-51aa-45b5-c379-28648db21e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав\n",
            "      LOC────    LOC──── PER───────                                   \n",
            " о решении властей Львовской области объявить 2019 год годом лидера \n",
            "                   LOC──────────────                                \n",
            "запрещенной в России Организации украинских националистов (ОУН) \n",
            "              LOC─── ORG─────────────────────────────────────── \n",
            "Степана Бандеры. Свое заявление он разместил в Twitter. «Я не могу \n",
            "PER────────────                                ORG────             \n",
            "понять, как прославление тех, кто непосредственно принимал участие в \n",
            "ужасных антисемитских преступлениях, помогает бороться с \n",
            "антисемитизмом и ксенофобией. Украина не должна забывать о \n",
            "                              LOC────                      \n",
            "преступлениях, совершенных против украинских евреев, и никоим образом \n",
            "не отмечать их через почитание их исполнителей», — написал дипломат. \n",
            "11 декабря Львовский областной совет принял решение провозгласить 2019\n",
            "           ORG──────────────────────                                  \n",
            " год в регионе годом Степана Бандеры в связи с празднованием 110-летия\n",
            "                     PER────────────                                  \n",
            " со дня рождения лидера ОУН (Бандера родился 1 января 1909 года). В \n",
            "                        ORG                                         \n",
            "июле аналогичное решение принял Житомирский областной совет. В начале \n",
            "                                ORG────────────────────────           \n",
            "месяца с предложением к президенту страны Петру Порошенко вернуть \n",
            "                                          PER────────────         \n",
            "Бандере звание Героя Украины обратились депутаты Верховной Рады. \n",
            "PER────              LOC────                     ORG───────────  \n",
            "Парламентарии уверены, что признание Бандеры национальным героем \n",
            "                                     PER────                     \n",
            "поможет в борьбе с подрывной деятельностью против Украины в \n",
            "                                                  LOC────   \n",
            "информационном поле, а также остановит «распространение мифов, \n",
            "созданных российской пропагандой». Степан Бандера (1909-1959) был \n",
            "                                   PER───────────                 \n",
            "одним из лидеров Организации украинских националистов, выступающей за \n",
            "                 ORG─────────────────────────────────                 \n",
            "создание независимого государства на территориях с украиноязычным \n",
            "населением. В 2010 году в период президентства Виктора Ющенко Бандера \n",
            "                                               PER─────────── PER──── \n",
            "был посмертно признан Героем Украины, однако впоследствии это решение \n",
            "                             LOC────                                  \n",
            "было отменено судом. \n"
          ]
        }
      ],
      "source": [
        "doc.ner.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNtk45cBkVht",
        "outputId": "3661d052-9b4f-4433-9b79-ae38f3068e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               Посол NOUN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n",
            "             Израиля PROPN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
            "                  на ADP\n",
            "             Украине PROPN|Animacy=Inan|Case=Loc|Gender=Fem|Number=Sing\n",
            "               Йоэль PROPN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n",
            "                Лион PROPN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n",
            "           признался VERB|Aspect=Perf|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Mid\n",
            "                   , PUNCT\n",
            "                 что SCONJ\n",
            "              пришел VERB|Aspect=Perf|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act\n",
            "                   в ADP\n",
            "                 шок NOUN|Animacy=Inan|Case=Acc|Gender=Masc|Number=Sing\n",
            "                   , PUNCT\n",
            "               узнав VERB|Aspect=Perf|Tense=Past|VerbForm=Conv|Voice=Act\n",
            "                   о ADP\n",
            "             решении NOUN|Animacy=Inan|Case=Loc|Gender=Neut|Number=Sing\n",
            "             властей NOUN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Plur\n",
            "           Львовской ADJ|Case=Gen|Degree=Pos|Gender=Fem|Number=Sing\n",
            "             области NOUN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n",
            "            объявить VERB|Aspect=Perf|VerbForm=Inf|Voice=Act\n",
            "                2019 ADJ\n",
            "                 год NOUN|Animacy=Inan|Case=Acc|Gender=Masc|Number=Sing\n",
            "               годом NOUN|Animacy=Inan|Case=Ins|Gender=Masc|Number=Sing\n",
            "              лидера NOUN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Sing\n",
            "         запрещенной VERB|Aspect=Perf|Case=Gen|Gender=Fem|Number=Sing|Tense=Past|VerbForm=Part|Voice=Pass\n",
            "                   в ADP\n",
            "              России PROPN|Animacy=Inan|Case=Loc|Gender=Fem|Number=Sing\n",
            "         Организации PROPN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n",
            "          украинских ADJ|Case=Gen|Degree=Pos|Number=Plur\n",
            "       националистов NOUN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Plur\n",
            "                   ( PUNCT\n",
            "                 ОУН PROPN|Animacy=Inan|Case=Nom|Gender=Fem|Number=Sing\n",
            "                   ) PUNCT\n",
            "             Степана PROPN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Sing\n",
            "             Бандеры PROPN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Sing\n",
            "                   . PUNCT\n"
          ]
        }
      ],
      "source": [
        "sent = doc.sents[0]\n",
        "sent.morph.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6-GjyMakn5Z",
        "outputId": "be0ad317-f6c4-4f4c-bc25-6fbab468058f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        ┌──► Посол         nsubj\n",
            "        │    Израиля       \n",
            "        │ ┌► на            case\n",
            "        │ └─ Украине       \n",
            "        │ ┌─ Йоэль         \n",
            "        │ └► Лион          flat:name\n",
            "┌─────┌─└─── признался     \n",
            "│     │ ┌──► ,             punct\n",
            "│     │ │ ┌► что           mark\n",
            "│     └►└─└─ пришел        ccomp\n",
            "│     │   ┌► в             case\n",
            "│     └──►└─ шок           obl\n",
            "│         ┌► ,             punct\n",
            "│ ┌────►┌─└─ узнав         advcl\n",
            "│ │     │ ┌► о             case\n",
            "│ │ ┌───└►└─ решении       obl\n",
            "│ │ │ ┌─└──► властей       nmod\n",
            "│ │ │ │   ┌► Львовской     amod\n",
            "│ │ │ └──►└─ области       nmod\n",
            "│ └─└►┌─┌─── объявить      nmod\n",
            "│     │ │ ┌► 2019          amod\n",
            "│     │ └►└─ год           obj\n",
            "│     └──►┌─ годом         obl\n",
            "│   ┌─────└► лидера        nmod\n",
            "│   │ ┌►┌─── запрещенной   acl\n",
            "│   │ │ │ ┌► в             case\n",
            "│   │ │ └►└─ России        obl\n",
            "│ ┌─└►└─┌─── Организации   nmod\n",
            "│ │     │ ┌► украинских    amod\n",
            "│ │   ┌─└►└─ националистов nmod\n",
            "│ │   │   ┌► (             punct\n",
            "│ │   └►┌─└─ ОУН           parataxis\n",
            "│ │     └──► )             punct\n",
            "│ └──────►┌─ Степана       appos\n",
            "│         └► Бандеры       flat:name\n",
            "└──────────► .             punct\n"
          ]
        }
      ],
      "source": [
        "sent.syntax.print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpv9bhXjadT9"
      },
      "source": [
        "# Применение нейронных сетей в задачах обработки текстов\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZt_Ch6L7VCr"
      },
      "outputs": [],
      "source": [
        "# Импортируем TensorFlow, Keras, NLTK и вспомогательные библиотеки\n",
        "\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRRBNKvznVyK",
        "outputId": "348f5844-b2b2-44cb-e9eb-9950ae564da5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Скачиваем список\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbREEdqPn-R-",
        "outputId": "03b82acc-cabd-4c41-e705-e38c36b87bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ],
      "source": [
        "# Подключаем датасет\n",
        "newsgroup_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "newsgroup_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
        "print(newsgroup_train.target_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ksG0KpQoVKt"
      },
      "outputs": [],
      "source": [
        "# создаем функции обработки текста\n",
        "def get_wordnet_pos (tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "def lemmatize (word_list):\n",
        "    wl = WordNetLemmatizer()\n",
        "    word_pos_tags = pos_tag(word_list)\n",
        "    lemmatized_list = []\n",
        "    for tag in word_pos_tags:\n",
        "        lemmatize_word = wl.lemmatize(tag[0],get_wordnet_pos(tag[1]))\n",
        "        lemmatized_list.append(lemmatize_word)\n",
        "    return \" \".join(lemmatized_list)\n",
        "\n",
        "def clean_text (text):\n",
        "    # Remove Pre and Post Spaces\n",
        "    text = str(text).strip()\n",
        "\n",
        "    # Lower case the entire text\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Substitute New Line Characters with spaces\n",
        "    text = re.sub(r\"\\n\", r\" \", text)\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    word_tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove the punctuation and  special characters from each individual word\n",
        "    cleaned_text = []\n",
        "    for word in word_tokens:\n",
        "        cleaned_text.append(\"\".join([char for char in word if char.isalnum()]))\n",
        "\n",
        "    # Specify the stop words list\n",
        "    stop_words = stopwords.words('english')\n",
        "\n",
        "    # Remove the stopwords and words containing less then 2 characters\n",
        "    text_tokens = [word for word in cleaned_text if (len(word) > 2) and (word not in stop_words)]\n",
        "\n",
        "    #Lemmatize each word in the word list\n",
        "    text = lemmatize (text_tokens)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_Zq3JTHox4V"
      },
      "outputs": [],
      "source": [
        "# подготовка обучающей и тестовой выборки\n",
        "X_train = np.array([clean_text(message) for message in newsgroup_train.data])\n",
        "y_train = np.array(newsgroup_train.target)\n",
        "X_test =  np.array([clean_text(message) for message in newsgroup_test.data])\n",
        "y_test =  np.array(newsgroup_test.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfIUMtvf7vA2",
        "outputId": "939fbbdf-a53a-4433-8b1b-72b631d0f91f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: (11314,)\n",
            "X_test: (7532,)\n",
            "y_train: (11314,)\n",
            "y_test: (7532,)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_test:\", y_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oMj6rA9rjUw",
        "outputId": "2b67f01a-9349-4836-8495-ac3b3b92a8fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{1: 'line',\n",
              " 2: 'subject',\n",
              " 3: 'organization',\n",
              " 4: 'would',\n",
              " 5: 'one',\n",
              " 6: 'write',\n",
              " 7: 'use',\n",
              " 8: 'get',\n",
              " 9: 'say',\n",
              " 10: 'article',\n",
              " 11: 'know',\n",
              " 12: 'people',\n",
              " 13: 'like',\n",
              " 14: 'make',\n",
              " 15: 'think',\n",
              " 16: 'university',\n",
              " 17: 'time',\n",
              " 18: 'nntppostinghost',\n",
              " 19: 'max',\n",
              " 20: 'well',\n",
              " 21: 'good',\n",
              " 22: 'also',\n",
              " 23: 'see',\n",
              " 24: 'new',\n",
              " 25: 'work',\n",
              " 26: 'system',\n",
              " 27: 'could',\n",
              " 28: 'take',\n",
              " 29: 'year',\n",
              " 30: 'want',\n",
              " 31: 'go',\n",
              " 32: 'right',\n",
              " 33: 'need',\n",
              " 34: 'come',\n",
              " 35: 'even',\n",
              " 36: 'thing',\n",
              " 37: 'problem',\n",
              " 38: 'way',\n",
              " 39: 'may',\n",
              " 40: 'look',\n",
              " 41: 'give',\n",
              " 42: 'god',\n",
              " 43: 'file',\n",
              " 44: 'find',\n",
              " 45: 'many',\n",
              " 46: 'state',\n",
              " 47: 'first',\n",
              " 48: 'two',\n",
              " 49: 'much',\n",
              " 50: 'question',\n",
              " 51: 'distribution',\n",
              " 52: 'try',\n",
              " 53: 'call',\n",
              " 54: 'point',\n",
              " 55: 'program',\n",
              " 56: 'run',\n",
              " 57: 'world',\n",
              " 58: 'anyone',\n",
              " 59: 'post',\n",
              " 60: 'drive',\n",
              " 61: 'believe',\n",
              " 62: 'tell',\n",
              " 63: 'mean',\n",
              " 64: 'seem',\n",
              " 65: 'number',\n",
              " 66: 'computer',\n",
              " 67: 'help',\n",
              " 68: 'please',\n",
              " 69: 'something',\n",
              " 70: 'window',\n",
              " 71: 'really',\n",
              " 72: 'include',\n",
              " 73: 'read',\n",
              " 74: 'back',\n",
              " 75: 'since',\n",
              " 76: 'day',\n",
              " 77: 'case',\n",
              " 78: 'email',\n",
              " 79: 'still',\n",
              " 80: 'information',\n",
              " 81: 'game',\n",
              " 82: 'key',\n",
              " 83: 'law',\n",
              " 84: 'government',\n",
              " 85: 'part',\n",
              " 86: 'start',\n",
              " 87: 'last',\n",
              " 88: 'must',\n",
              " 89: 'group',\n",
              " 90: 'thanks',\n",
              " 91: 'usa',\n",
              " 92: 'never',\n",
              " 93: 'let',\n",
              " 94: 'ask',\n",
              " 95: 'might',\n",
              " 96: 'replyto',\n",
              " 97: 'car',\n",
              " 98: 'support',\n",
              " 99: 'another',\n",
              " 100: 'sure',\n",
              " 101: 'without',\n",
              " 102: 'follow',\n",
              " 103: 'space',\n",
              " 104: 'version',\n",
              " 105: 'set',\n",
              " 106: 'name',\n",
              " 107: 'david',\n",
              " 108: 'etc',\n",
              " 109: 'keep',\n",
              " 110: 'long',\n",
              " 111: 'power',\n",
              " 112: 'put',\n",
              " 113: 'fact',\n",
              " 114: 'data',\n",
              " 115: 'science',\n",
              " 116: 'someone',\n",
              " 117: 'great',\n",
              " 118: 'available',\n",
              " 119: 'do',\n",
              " 120: 'reason',\n",
              " 121: 'list',\n",
              " 122: 'card',\n",
              " 123: 'send',\n",
              " 124: 'team',\n",
              " 125: 'lot',\n",
              " 126: 'show',\n",
              " 127: 'change',\n",
              " 128: 'high',\n",
              " 129: 'christian',\n",
              " 130: 'gun',\n",
              " 131: 'little',\n",
              " 132: 'john',\n",
              " 133: 'chip',\n",
              " 134: 'bad',\n",
              " 135: 'place',\n",
              " 136: 'however',\n",
              " 137: 'play',\n",
              " 138: 'software',\n",
              " 139: 'opinion',\n",
              " 140: 'anything',\n",
              " 141: 'around',\n",
              " 142: 'every',\n",
              " 143: 'probably',\n",
              " 144: 'course',\n",
              " 145: 'leave',\n",
              " 146: 'best',\n",
              " 147: 'true',\n",
              " 148: 'word',\n",
              " 149: 'consider',\n",
              " 150: 'book',\n",
              " 151: 'happen',\n",
              " 152: 'end',\n",
              " 153: 'life',\n",
              " 154: 'old',\n",
              " 155: 'public',\n",
              " 156: 'technology',\n",
              " 157: 'least',\n",
              " 158: 'second',\n",
              " 159: 'different',\n",
              " 160: 'kill',\n",
              " 161: 'talk',\n",
              " 162: 'bit',\n",
              " 163: 'claim',\n",
              " 164: 'live',\n",
              " 165: 'enough',\n",
              " 166: 'order',\n",
              " 167: 'note',\n",
              " 168: 'center',\n",
              " 169: 'research',\n",
              " 170: 'provide',\n",
              " 171: 'image',\n",
              " 172: 'base',\n",
              " 173: 'writes',\n",
              " 174: 'buy',\n",
              " 175: 'jesus',\n",
              " 176: 'control',\n",
              " 177: '1993',\n",
              " 178: 'idea',\n",
              " 179: 'message',\n",
              " 180: 'hard',\n",
              " 181: 'source',\n",
              " 182: 'service',\n",
              " 183: 'issue',\n",
              " 184: 'armenian',\n",
              " 185: 'far',\n",
              " 186: 'possible',\n",
              " 187: 'actually',\n",
              " 188: 'example',\n",
              " 189: 'either',\n",
              " 190: 'though',\n",
              " 191: 'big',\n",
              " 192: 'inc',\n",
              " 193: 'real',\n",
              " 194: 'answer',\n",
              " 195: 'cause',\n",
              " 196: 'b8f',\n",
              " 197: 'person',\n",
              " 198: 'child',\n",
              " 199: 'rather',\n",
              " 200: 'nothing',\n",
              " 201: 'mail',\n",
              " 202: 'next',\n",
              " 203: 'mark',\n",
              " 204: 'driver',\n",
              " 205: 'internet',\n",
              " 206: 'else',\n",
              " 207: 'machine',\n",
              " 208: 'american',\n",
              " 209: 'wrong',\n",
              " 210: 'standard',\n",
              " 211: 'free',\n",
              " 212: 'access',\n",
              " 213: 'man',\n",
              " 214: 'address',\n",
              " 215: 'exist',\n",
              " 216: 'phone',\n",
              " 217: 'large',\n",
              " 218: 'a86',\n",
              " 219: 'build',\n",
              " 220: 'allow',\n",
              " 221: 'yes',\n",
              " 222: 'human',\n",
              " 223: 'disk',\n",
              " 224: 'maybe',\n",
              " 225: 'win',\n",
              " 226: 'bill',\n",
              " 227: 'national',\n",
              " 228: 'player',\n",
              " 229: 'code',\n",
              " 230: 'able',\n",
              " 231: 'hand',\n",
              " 232: 'user',\n",
              " 233: 'others',\n",
              " 234: 'always',\n",
              " 235: 'turn',\n",
              " 236: 'hear',\n",
              " 237: 'report',\n",
              " 238: 'price',\n",
              " 239: 'info',\n",
              " 240: 'type',\n",
              " 241: 'keywords',\n",
              " 242: 'require',\n",
              " 243: 'kind',\n",
              " 244: 'several',\n",
              " 245: 'today',\n",
              " 246: 'general',\n",
              " 247: 'israel',\n",
              " 248: 'small',\n",
              " 249: 'home',\n",
              " 250: 'area',\n",
              " 251: 'yet',\n",
              " 252: 'sound',\n",
              " 253: 'less',\n",
              " 254: 'view',\n",
              " 255: 'quite',\n",
              " 256: 'ever',\n",
              " 257: '145',\n",
              " 258: 'sale',\n",
              " 259: 'three',\n",
              " 260: 'pay',\n",
              " 261: 'result',\n",
              " 262: 'cost',\n",
              " 263: 'sell',\n",
              " 264: 'become',\n",
              " 265: 'away',\n",
              " 266: 'open',\n",
              " 267: 'application',\n",
              " 268: 'week',\n",
              " 269: 'test',\n",
              " 270: 'remember',\n",
              " 271: 'speed',\n",
              " 272: 'check',\n",
              " 273: 'move',\n",
              " 274: 'news',\n",
              " 275: 'company',\n",
              " 276: 'create',\n",
              " 277: 'study',\n",
              " 278: 'color',\n",
              " 279: 'president',\n",
              " 280: 'hold',\n",
              " 281: 'country',\n",
              " 282: 'whether',\n",
              " 283: 'current',\n",
              " 284: 'steve',\n",
              " 285: 'mac',\n",
              " 286: 'side',\n",
              " 287: 'feel',\n",
              " 288: 'design',\n",
              " 289: 'encryption',\n",
              " 290: 'agree',\n",
              " 291: 'already',\n",
              " 292: 'money',\n",
              " 293: 'michael',\n",
              " 294: 'war',\n",
              " 295: 'understand',\n",
              " 296: 'department',\n",
              " 297: 'evidence',\n",
              " 298: 'netcomcom',\n",
              " 299: 'value',\n",
              " 300: 'force',\n",
              " 301: 'display',\n",
              " 302: 'institute',\n",
              " 303: 'rule',\n",
              " 304: 'argument',\n",
              " 305: 'graphic',\n",
              " 306: 'assume',\n",
              " 307: 'matter',\n",
              " 308: 'lead',\n",
              " 309: 'love',\n",
              " 310: 'stop',\n",
              " 311: 'box',\n",
              " 312: 'offer',\n",
              " 313: 'local',\n",
              " 314: 'ago',\n",
              " 315: 'jew',\n",
              " 316: 'apr',\n",
              " 317: 'low',\n",
              " 318: 'mention',\n",
              " 319: 'city',\n",
              " 320: 'add',\n",
              " 321: 'bible',\n",
              " 322: 'server',\n",
              " 323: 'perhaps',\n",
              " 324: 'copy',\n",
              " 325: 'memory',\n",
              " 326: 'experience',\n",
              " 327: 'house',\n",
              " 328: 'robert',\n",
              " 329: 'woman',\n",
              " 330: 'fax',\n",
              " 331: 'clipper',\n",
              " 332: 'act',\n",
              " 333: 'hope',\n",
              " 334: 'package',\n",
              " 335: 'guy',\n",
              " 336: 'difference',\n",
              " 337: 'care',\n",
              " 338: 'mind',\n",
              " 339: 'whole',\n",
              " 340: 'close',\n",
              " 341: 'pretty',\n",
              " 342: 'lose',\n",
              " 343: 'april',\n",
              " 344: 'stuff',\n",
              " 345: 'interest',\n",
              " 346: 'mike',\n",
              " 347: 'return',\n",
              " 348: 'attack',\n",
              " 349: 'paul',\n",
              " 350: 'begin',\n",
              " 351: 'network',\n",
              " 352: 'job',\n",
              " 353: 'communication',\n",
              " 354: 'die',\n",
              " 355: 'expect',\n",
              " 356: 'member',\n",
              " 357: 'jim',\n",
              " 358: 'church',\n",
              " 359: 'deal',\n",
              " 360: 'carry',\n",
              " 361: 'contact',\n",
              " 362: 'israeli',\n",
              " 363: 'turkish',\n",
              " 364: 'interested',\n",
              " 365: 'appear',\n",
              " 366: 'device',\n",
              " 367: 'religion',\n",
              " 368: 'head',\n",
              " 369: 'sun',\n",
              " 370: 'death',\n",
              " 371: 'bike',\n",
              " 372: 'save',\n",
              " 373: 'canada',\n",
              " 374: 'model',\n",
              " 375: 'everything',\n",
              " 376: 'product',\n",
              " 377: 'important',\n",
              " 378: 'month',\n",
              " 379: 'comment',\n",
              " 380: 'accept',\n",
              " 381: 'school',\n",
              " 382: 'everyone',\n",
              " 383: 'fire',\n",
              " 384: 'error',\n",
              " 385: 'fast',\n",
              " 386: 'hit',\n",
              " 387: 'rate',\n",
              " 388: 'level',\n",
              " 389: 'light',\n",
              " 390: 'original',\n",
              " 391: 'easy',\n",
              " 392: 'action',\n",
              " 393: 'truth',\n",
              " 394: '1d9',\n",
              " 395: 'guess',\n",
              " 396: 'often',\n",
              " 397: 'white',\n",
              " 398: 'almost',\n",
              " 399: 'sort',\n",
              " 400: 'monitor',\n",
              " 401: 'scsi',\n",
              " 402: 'reference',\n",
              " 403: 'articleid',\n",
              " 404: 'effect',\n",
              " 405: 'advance',\n",
              " 406: 'form',\n",
              " 407: 'simply',\n",
              " 408: 'friend',\n",
              " 409: 'format',\n",
              " 410: 'weapon',\n",
              " 411: 'speak',\n",
              " 412: 'full',\n",
              " 413: 'video',\n",
              " 414: 'body',\n",
              " 415: 'board',\n",
              " 416: 'engineering',\n",
              " 417: 'dept',\n",
              " 418: 'statement',\n",
              " 419: 'wonder',\n",
              " 420: 'bring',\n",
              " 421: 'cover',\n",
              " 422: 'season',\n",
              " 423: 'arm',\n",
              " 424: 'size',\n",
              " 425: 'position',\n",
              " 426: 'instead',\n",
              " 427: 'although',\n",
              " 428: 'certainly',\n",
              " 429: 'history',\n",
              " 430: 'division',\n",
              " 431: 'california',\n",
              " 432: 'plan',\n",
              " 433: 'anybody',\n",
              " 434: 'regard',\n",
              " 435: 'single',\n",
              " 436: 'couple',\n",
              " 437: 'ground',\n",
              " 438: 'anyway',\n",
              " 439: 'xnewsreader',\n",
              " 440: 'discussion',\n",
              " 441: 'college',\n",
              " 442: 'summary',\n",
              " 443: 'men',\n",
              " 444: 'later',\n",
              " 445: 'hell',\n",
              " 446: 'output',\n",
              " 447: 'suggest',\n",
              " 448: 'mode',\n",
              " 449: 'correct',\n",
              " 450: 'receive',\n",
              " 451: 'press',\n",
              " 452: 'event',\n",
              " 453: 'ftp',\n",
              " 454: 'explain',\n",
              " 455: 'sense',\n",
              " 456: 'project',\n",
              " 457: 'crime',\n",
              " 458: 'unless',\n",
              " 459: 'security',\n",
              " 460: 'black',\n",
              " 461: 'present',\n",
              " 462: 'drug',\n",
              " 463: 'break',\n",
              " 464: 'top',\n",
              " 465: 'appreciate',\n",
              " 466: 'function',\n",
              " 467: 'hockey',\n",
              " 468: 'process',\n",
              " 469: '100',\n",
              " 470: 'situation',\n",
              " 471: 'entry',\n",
              " 472: 'clinton',\n",
              " 473: 'release',\n",
              " 474: 'major',\n",
              " 475: 'similar',\n",
              " 476: 'unix',\n",
              " 477: 'reply',\n",
              " 478: 'site',\n",
              " 479: 'certain',\n",
              " 480: 'faith',\n",
              " 481: 'apple',\n",
              " 482: 'continue',\n",
              " 483: 'san',\n",
              " 484: 'earth',\n",
              " 485: 'net',\n",
              " 486: 'individual',\n",
              " 487: 'term',\n",
              " 488: 'purpose',\n",
              " 489: 'face',\n",
              " 490: 'clear',\n",
              " 491: 'period',\n",
              " 492: 'quote',\n",
              " 493: 'within',\n",
              " 494: 'request',\n",
              " 495: 'likely',\n",
              " 496: 'private',\n",
              " 497: 'road',\n",
              " 498: 'late',\n",
              " 499: 'police',\n",
              " 500: 'policy',\n",
              " 501: 'goal',\n",
              " 502: 'suppose',\n",
              " 503: 'figure',\n",
              " 504: 'jewish',\n",
              " 505: 'record',\n",
              " 506: 'learn',\n",
              " 507: 'office',\n",
              " 508: 'stand',\n",
              " 509: 'nice',\n",
              " 510: 'land',\n",
              " 511: 'date',\n",
              " 512: 'decide',\n",
              " 513: 'christ',\n",
              " 514: 'simple',\n",
              " 515: 'via',\n",
              " 516: 'windows',\n",
              " 517: 'faq',\n",
              " 518: 'usually',\n",
              " 519: 'screen',\n",
              " 520: 'hardware',\n",
              " 521: 'atheist',\n",
              " 522: 'protect',\n",
              " 523: 'strong',\n",
              " 524: 'exactly',\n",
              " 525: 'saw',\n",
              " 526: 'except',\n",
              " 527: 'involve',\n",
              " 528: 'young',\n",
              " 529: 'especially',\n",
              " 530: 'dave',\n",
              " 531: 'early',\n",
              " 532: 'heard',\n",
              " 533: 'response',\n",
              " 534: 'fan',\n",
              " 535: 'mine',\n",
              " 536: 'washington',\n",
              " 537: 'sorry',\n",
              " 538: 'section',\n",
              " 539: 'keith',\n",
              " 540: 'nasa',\n",
              " 541: 'york',\n",
              " 542: 'wait',\n",
              " 543: 'text',\n",
              " 544: 'detail',\n",
              " 545: 'tax',\n",
              " 546: 'per',\n",
              " 547: 'gmt',\n",
              " 548: 'society',\n",
              " 549: 'widget',\n",
              " 550: 'million',\n",
              " 551: 'pick',\n",
              " 552: 'short',\n",
              " 553: 'health',\n",
              " 554: 'corporation',\n",
              " 555: 'page',\n",
              " 556: 'watch',\n",
              " 557: 'tin',\n",
              " 558: 'bank',\n",
              " 559: 'fine',\n",
              " 560: 'dod',\n",
              " 561: 'common',\n",
              " 562: 'pittsburgh',\n",
              " 563: 'limit',\n",
              " 564: 'western',\n",
              " 565: 'business',\n",
              " 566: 'league',\n",
              " 567: 'thus',\n",
              " 568: 'night',\n",
              " 569: 'dead',\n",
              " 570: 'cut',\n",
              " 571: 'launch',\n",
              " 572: 'condition',\n",
              " 573: 'attempt',\n",
              " 574: 'radio',\n",
              " 575: 'story',\n",
              " 576: 'food',\n",
              " 577: 'increase',\n",
              " 578: 'particular',\n",
              " 579: 'bob',\n",
              " 580: 'brian',\n",
              " 581: 'manager',\n",
              " 582: 'cheap',\n",
              " 583: 'apply',\n",
              " 584: 'rest',\n",
              " 585: 'produce',\n",
              " 586: 'port',\n",
              " 587: 'among',\n",
              " 588: 'bus',\n",
              " 589: 'option',\n",
              " 590: 'ibm',\n",
              " 591: 'belief',\n",
              " 592: 'pass',\n",
              " 593: 'air',\n",
              " 594: 'political',\n",
              " 595: 'score',\n",
              " 596: 'james',\n",
              " 597: 'contain',\n",
              " 598: 'concern',\n",
              " 599: '2di',\n",
              " 600: 'water',\n",
              " 601: 'red',\n",
              " 602: 'mouse',\n",
              " 603: 'express',\n",
              " 604: 'handle',\n",
              " 605: 'fail',\n",
              " 606: 'command',\n",
              " 607: 'court',\n",
              " 608: 'define',\n",
              " 609: 'therefore',\n",
              " 610: 'chance',\n",
              " 611: 'moral',\n",
              " 612: 'method',\n",
              " 613: 'third',\n",
              " 614: 'tape',\n",
              " 615: 'accord',\n",
              " 616: 'future',\n",
              " 617: 'field',\n",
              " 618: 'whatever',\n",
              " 619: 'draw',\n",
              " 620: 'compare',\n",
              " 621: 'switch',\n",
              " 622: 'past',\n",
              " 623: 'military',\n",
              " 624: 'controller',\n",
              " 625: 'toronto',\n",
              " 626: 'smith',\n",
              " 627: 'paper',\n",
              " 628: 'king',\n",
              " 629: 'unit',\n",
              " 630: 'due',\n",
              " 631: 'bhj',\n",
              " 632: 'authority',\n",
              " 633: 'wire',\n",
              " 634: 'theory',\n",
              " 635: 'texas',\n",
              " 636: 'author',\n",
              " 637: 'anonymous',\n",
              " 638: 'develop',\n",
              " 639: 'miss',\n",
              " 640: 'front',\n",
              " 641: 'personal',\n",
              " 642: 'shot',\n",
              " 643: 'directory',\n",
              " 644: 'total',\n",
              " 645: 'engine',\n",
              " 646: 'tool',\n",
              " 647: 'object',\n",
              " 648: 'solution',\n",
              " 649: 'andrew',\n",
              " 650: 'four',\n",
              " 651: 'criminal',\n",
              " 652: 'library',\n",
              " 653: 'peter',\n",
              " 654: 'final',\n",
              " 655: 'frank',\n",
              " 656: 'sometimes',\n",
              " 657: 'flame',\n",
              " 658: 'special',\n",
              " 659: 'upon',\n",
              " 660: 'family',\n",
              " 661: 'medium',\n",
              " 662: 'specific',\n",
              " 663: 'murder',\n",
              " 664: 'voice',\n",
              " 665: 'federal',\n",
              " 666: 'ram',\n",
              " 667: 'bear',\n",
              " 668: 'tom',\n",
              " 669: 'recently',\n",
              " 670: 'chicago',\n",
              " 671: 'fall',\n",
              " 672: 'series',\n",
              " 673: 'algorithm',\n",
              " 674: 'sign',\n",
              " 675: 'agency',\n",
              " 676: 'worth',\n",
              " 677: 'describe',\n",
              " 678: 'trade',\n",
              " 679: 'resource',\n",
              " 680: 'soon',\n",
              " 681: 'baseball',\n",
              " 682: 'behind',\n",
              " 683: 'greek',\n",
              " 684: 'near',\n",
              " 685: 'secret',\n",
              " 686: 'judge',\n",
              " 687: 'richard',\n",
              " 688: 'letter',\n",
              " 689: 'class',\n",
              " 690: 'along',\n",
              " 691: 'together',\n",
              " 692: 'choose',\n",
              " 693: 'international',\n",
              " 694: 'motif',\n",
              " 695: 'plus',\n",
              " 696: 'font',\n",
              " 697: 'complete',\n",
              " 698: 'wish',\n",
              " 699: 'scott',\n",
              " 700: 'muslim',\n",
              " 701: 'interface',\n",
              " 702: 'party',\n",
              " 703: 'technical',\n",
              " 704: 'religious',\n",
              " 705: 'feature',\n",
              " 706: 'official',\n",
              " 707: 'share',\n",
              " 708: 'station',\n",
              " 709: 'citizen',\n",
              " 710: 'giz',\n",
              " 711: 'lie',\n",
              " 712: 'amount',\n",
              " 713: 'peace',\n",
              " 714: 'previous',\n",
              " 715: 'firearm',\n",
              " 716: 'account',\n",
              " 717: 'delete',\n",
              " 718: '1992',\n",
              " 719: 'doubt',\n",
              " 720: 'meet',\n",
              " 721: 'prove',\n",
              " 722: 'father',\n",
              " 723: 'arab',\n",
              " 724: 'legal',\n",
              " 725: 'administration',\n",
              " 726: 'russian',\n",
              " 727: 'picture',\n",
              " 728: 'market',\n",
              " 729: 'approach',\n",
              " 730: 'various',\n",
              " 731: 'laboratory',\n",
              " 732: 'privacy',\n",
              " 733: 'necessary',\n",
              " 734: 'compute',\n",
              " 735: 'sin',\n",
              " 736: 'knowledge',\n",
              " 737: 'block',\n",
              " 738: 'occur',\n",
              " 739: 'development',\n",
              " 740: 'disclaimer',\n",
              " 741: 'manual',\n",
              " 742: 'minute',\n",
              " 743: 'medical',\n",
              " 744: 'currently',\n",
              " 745: 'choice',\n",
              " 746: 'nhl',\n",
              " 747: 'performance',\n",
              " 748: 'average',\n",
              " 749: 'slow',\n",
              " 750: 'printer',\n",
              " 751: 'notice',\n",
              " 752: 'thought',\n",
              " 753: 'burn',\n",
              " 754: 'fix',\n",
              " 755: 'age',\n",
              " 756: 'chris',\n",
              " 757: 'cable',\n",
              " 758: 'avoid',\n",
              " 759: 'otherwise',\n",
              " 760: 'population',\n",
              " 761: 'north',\n",
              " 762: 'thank',\n",
              " 763: 'insurance',\n",
              " 764: 'forget',\n",
              " 765: 'supply',\n",
              " 766: 'quality',\n",
              " 767: 'defense',\n",
              " 768: 'replace',\n",
              " 769: 'title',\n",
              " 770: 'remove',\n",
              " 771: 'thomas',\n",
              " 772: 'germany',\n",
              " 773: 'none',\n",
              " 774: 'spend',\n",
              " 775: 'outside',\n",
              " 776: 'univ',\n",
              " 777: 'operation',\n",
              " 778: 'hour',\n",
              " 779: 'ide',\n",
              " 780: 'owner',\n",
              " 781: 'effort',\n",
              " 782: 'clearly',\n",
              " 783: 'fight',\n",
              " 784: 'fit',\n",
              " 785: 'charge',\n",
              " 786: 'son',\n",
              " 787: 'community',\n",
              " 788: 'doctor',\n",
              " 789: 'freedom',\n",
              " 790: 'christianity',\n",
              " 791: 'shall',\n",
              " 792: 'remain',\n",
              " 793: 'eric',\n",
              " 794: 'united',\n",
              " 795: 'language',\n",
              " 796: 'input',\n",
              " 797: 'objective',\n",
              " 798: 'stay',\n",
              " 799: 'serial',\n",
              " 800: 'modem',\n",
              " 801: 'fbi',\n",
              " 802: 'purchase',\n",
              " 803: 'pat',\n",
              " 804: 'vote',\n",
              " 805: 'document',\n",
              " 806: 'activity',\n",
              " 807: 'load',\n",
              " 808: 'sit',\n",
              " 809: 'online',\n",
              " 810: 'serious',\n",
              " 811: 'realize',\n",
              " 812: 'america',\n",
              " 813: 'publish',\n",
              " 814: 'print',\n",
              " 815: 'search',\n",
              " 816: 'basic',\n",
              " 817: 'practice',\n",
              " 818: 'prevent',\n",
              " 819: 'main',\n",
              " 820: 'convert',\n",
              " 821: 'newsgroup',\n",
              " 822: 'digital',\n",
              " 823: 'refer',\n",
              " 824: 'eye',\n",
              " 825: 'george',\n",
              " 826: 'morality',\n",
              " 827: 'willing',\n",
              " 828: 'commercial',\n",
              " 829: 'keyboard',\n",
              " 830: 'gas',\n",
              " 831: 'count',\n",
              " 832: 'street',\n",
              " 833: 'gary',\n",
              " 834: 'kid',\n",
              " 835: 'completely',\n",
              " 836: 'armenia',\n",
              " 837: 'blue',\n",
              " 838: 'gordon',\n",
              " 839: 'student',\n",
              " 840: 'drop',\n",
              " 841: 'jon',\n",
              " 842: 'inside',\n",
              " 843: 'turkey',\n",
              " 844: 'ship',\n",
              " 845: 'boston',\n",
              " 846: 'half',\n",
              " 847: 'safety',\n",
              " 848: 'depend',\n",
              " 849: 'satellite',\n",
              " 850: 'orbit',\n",
              " 851: 'serve',\n",
              " 852: 'grant',\n",
              " 853: 'nature',\n",
              " 854: 'decision',\n",
              " 855: 'lack',\n",
              " 856: 'existence',\n",
              " 857: 'respond',\n",
              " 858: 'material',\n",
              " 859: 'suggestion',\n",
              " 860: 'normal',\n",
              " 861: 'tim',\n",
              " 862: 'determine',\n",
              " 863: 'secure',\n",
              " 864: 'mass',\n",
              " 865: 'south',\n",
              " 866: 'dan',\n",
              " 867: 'argue',\n",
              " 868: 'disease',\n",
              " 869: 'reach',\n",
              " 870: 'beat',\n",
              " 871: 'stephanopoulos',\n",
              " 872: 'corp',\n",
              " 873: 'lab',\n",
              " 874: 'scientific',\n",
              " 875: 'transfer',\n",
              " 876: 'mile',\n",
              " 877: 'trust',\n",
              " 878: 'thousand',\n",
              " 879: 'range',\n",
              " 880: 'connect',\n",
              " 881: 'fund',\n",
              " 882: 'indeed',\n",
              " 883: 'congress',\n",
              " 884: 'finally',\n",
              " 885: 'obtain',\n",
              " 886: 'adam',\n",
              " 887: 'archive',\n",
              " 888: 'dealer',\n",
              " 889: 'uunet',\n",
              " 890: 'room',\n",
              " 891: 'lord',\n",
              " 892: 'useful',\n",
              " 893: 'throw',\n",
              " 894: 'star',\n",
              " 895: 'mission',\n",
              " 896: 'turk',\n",
              " 897: 'easily',\n",
              " 898: 'matthew',\n",
              " 899: 'door',\n",
              " 900: 'inreplyto',\n",
              " 901: 'msg',\n",
              " 902: 'definition',\n",
              " 903: 'reasonable',\n",
              " 904: 'west',\n",
              " 905: 'rid',\n",
              " 906: 'generally',\n",
              " 907: 'advice',\n",
              " 908: 'happy',\n",
              " 909: 'obviously',\n",
              " 910: 'moon',\n",
              " 911: 'floppy',\n",
              " 912: 'intend',\n",
              " 913: 'raise',\n",
              " 914: 'internal',\n",
              " 915: 'usenet',\n",
              " 916: 'amendment',\n",
              " 917: 'directly',\n",
              " 918: 'ten',\n",
              " 919: 'nation',\n",
              " 920: 'discuss',\n",
              " 921: 'difficult',\n",
              " 922: 'education',\n",
              " 923: 'stupid',\n",
              " 924: 'wing',\n",
              " 925: '550',\n",
              " 926: 'addition',\n",
              " 927: 'necessarily',\n",
              " 928: 'reserve',\n",
              " 929: 'illinois',\n",
              " 930: 'respect',\n",
              " 931: 'conference',\n",
              " 932: 'doug',\n",
              " 933: 'magazine',\n",
              " 934: 'direct',\n",
              " 935: 'character',\n",
              " 936: 'shoot',\n",
              " 937: 'unfortunately',\n",
              " 938: 'instal',\n",
              " 939: 'vehicle',\n",
              " 940: 'license',\n",
              " 941: 'los',\n",
              " 942: 'blood',\n",
              " 943: 'enforcement',\n",
              " 944: 'imagine',\n",
              " 945: 'basis',\n",
              " 946: 'henry',\n",
              " 947: 'store',\n",
              " 948: 'joe',\n",
              " 949: 'trouble',\n",
              " 950: 'obvious',\n",
              " 951: 'entire',\n",
              " 952: 'playoff',\n",
              " 953: 'somebody',\n",
              " 954: 'reduce',\n",
              " 955: 'signal',\n",
              " 956: 'roger',\n",
              " 957: 'measure',\n",
              " 958: 'oil',\n",
              " 959: 'conclusion',\n",
              " 960: 'east',\n",
              " 961: 'circuit',\n",
              " 962: 'wife',\n",
              " 963: 'electronic',\n",
              " 964: 'folk',\n",
              " 965: 'neither',\n",
              " 966: 'heart',\n",
              " 967: 'aid',\n",
              " 968: 'item',\n",
              " 969: 'evil',\n",
              " 970: 'associate',\n",
              " 971: 'pull',\n",
              " 972: 'colorado',\n",
              " 973: 'hole',\n",
              " 974: 'trial',\n",
              " 975: 'excellent',\n",
              " 976: 'apparently',\n",
              " 977: 'risk',\n",
              " 978: 'link',\n",
              " 979: 'recent',\n",
              " 980: 'park',\n",
              " 981: 'stick',\n",
              " 982: 'suspect',\n",
              " 983: 'ride',\n",
              " 984: 'client',\n",
              " 985: 'dog',\n",
              " 986: 'van',\n",
              " 987: 'alone',\n",
              " 988: 'upgrade',\n",
              " 989: 'round',\n",
              " 990: 'step',\n",
              " 991: 'originator',\n",
              " 992: 'ron',\n",
              " 993: 'suffer',\n",
              " 994: 'environment',\n",
              " 995: 'appropriate',\n",
              " 996: 'whose',\n",
              " 997: 'soldier',\n",
              " 998: 'ability',\n",
              " 999: 'commit',\n",
              " 1000: 'ken',\n",
              " ...}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# токенизация\n",
        "tokenizer = Tokenizer(num_words=100000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "tokenizer.index_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qb8RGI2sk3Q",
        "outputId": "e72d1bbf-b711-46dd-a092-5ffcbc14ddc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab Size: 148442\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.index_word) + 1\n",
        "print('Vocab Size:', vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NSV174LvWV1",
        "outputId": "1eda63c0-1f08-43e5-9c7d-c4ee995ce858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-24 09:24:32--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:26:42--  (try: 2)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:28:54--  (try: 3)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:31:06--  (try: 4)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:33:23--  (try: 5)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:35:39--  (try: 6)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:37:55--  (try: 7)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:40:11--  (try: 8)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:42:31--  (try: 9)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:44:52--  (try:10)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:47:12--  (try:11)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:49:31--  (try:12)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:51:50--  (try:13)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:54:10--  (try:14)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:56:29--  (try:15)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 09:58:48--  (try:16)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 10:01:07--  (try:17)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 10:03:27--  (try:18)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 10:05:46--  (try:19)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Retrying.\n",
            "\n",
            "--2024-04-24 10:08:05--  (try:20)  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... failed: Connection timed out.\n",
            "Giving up.\n",
            "\n",
            "unzip:  cannot find or open glove.6B.zip, glove.6B.zip.zip or glove.6B.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# Загружаем предобученные эмбеддинги\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "778k3OZJxK-T"
      },
      "outputs": [],
      "source": [
        "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_token = tokenizer.texts_to_sequences(X_test)\n",
        "sequence_len = 300\n",
        "X_train_token = pad_sequences(X_train_token, padding='post', maxlen=sequence_len)\n",
        "X_test_token = pad_sequences(X_test_token, padding='post', maxlen=sequence_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gsNQ6bkNs-fU"
      },
      "outputs": [],
      "source": [
        "# Формируем структуру нейронной сети\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_len))\n",
        "model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(20, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vAjoIOYEkIU0"
      },
      "outputs": [],
      "source": [
        "# Обучение модели\n",
        "history = model.fit(X_train_token, y_train, epochs=10, validation_data=(X_test_token, y_test), batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nYdvE_n6DOF1"
      },
      "outputs": [],
      "source": [
        "metrics_df = pd.DataFrame(history.history)\n",
        "print(metrics_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zI4rV8-k6IO_"
      },
      "outputs": [],
      "source": [
        "token = tokenizer.texts_to_sequences(['launch spaceship moon'])\n",
        "token_pad = pad_sequences(token, padding='post', maxlen=sequence_len)\n",
        "newsgroup_train.target_names[model.predict(token_pad).argmax()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4ruCKpH5CvBc"
      },
      "outputs": [],
      "source": [
        "token = tokenizer.texts_to_sequences(['God life death church'])\n",
        "token_pad = pad_sequences(token, padding='post', maxlen=sequence_len)\n",
        "newsgroup_train.target_names[model.predict(token_pad).argmax()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irHNgaNU7q7q"
      },
      "source": [
        " Провести категоризацию произвольного текста с помощью данной модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n7F-fovWmUCi"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Загрузка предварительно обученной модели BERT и токенизатора\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "# Подготовка текста для категоризации\n",
        "text = \"Интересы Александра Михайловича Прохорова в физике были разнообразны, но в какой-то момент он сосредоточился на теме лазеров. Их существование было ранее теоретически предсказано А. Эйнштейном. Именно группа Прохорова-Басова и Таунс, находясь по разные стороны земного шара, независимо друг от друга разработали теорию для создания первых мазеров (микроволновые излучатели) и лазеров (излучатели видимого света). Новый принцип действия генераторов, разработанный Прохоровым, послужил основой для создания генераторов непрерывного действия. Большой вклад он внёс и в развитие лазерного термоядерного синтеза.\"\n",
        "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "# Предсказание категории текста\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    predicted_category = torch.argmax(probabilities).item()\n",
        "\n",
        "# Вывод результата\n",
        "categories = ['научный', 'художественный', 'автобиографический']\n",
        "print(f\"Predicted category: {categories[predicted_category]}\")\n",
        "print(\"Probabilities:\")\n",
        "for i, prob in enumerate(probabilities.squeeze().tolist()):\n",
        "    print(f\"{categories[i]}: {np.round(prob * 100, 2)}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}